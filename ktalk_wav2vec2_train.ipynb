{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f554c903-777c-4560-9cbb-92843d96096b",
   "metadata": {},
   "source": [
    "# 환경 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce4f441d-1cb7-4ef9-ab8f-408f8e593335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setproctitle import setproctitle\n",
    "setproctitle(\"ktalk_Wav2Vec2 textfile\")\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_path = \"/wav2vec2/s-kr/fine-tune/dataset\"\n",
    "kspon_path = os.path.join(dataset_path, \"Broadcast-contents\") # 방소, 취미 등등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "556e6066-e208-4d50-bfef-2ef879644285",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d00e2674-d40d-4a01-aa82-df6394b9a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['WANDB_SILENT']=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93b5a220-95ed-4074-91e9-3d0fd44ab69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)\n",
    "print('Count of using GPUs:', torch.cuda.device_count()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff2e3f0-0c29-4501-8911-eb76b19f62b3",
   "metadata": {},
   "source": [
    "# KsponSpeech Vocab 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d541bb0d-93b7-4aaa-9119-9bff066828bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setproctitle import setproctitle\n",
    "setproctitle(\"aihub_Wav2Vec2 Vocab_mario\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, ClassLabel\n",
    "from IPython.display import display, HTML\n",
    "from glob import glob\n",
    "from kspon_preprocess import special_filter, bracket_filter\n",
    "\n",
    "import re\n",
    "import librosa\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def show_random_elements(dataset, num_examples=15):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    \n",
    "    df = pd.DataFrame(dataset[:num_examples])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "    \n",
    "def _read_txt_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    # with open(file_path, 'r', encoding='cp949') as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    return text\n",
    "\n",
    "\n",
    "text_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23670479-7cdc-4468-bf83-84fe5c87c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_attention_heads=8\n",
    "sr=8000\n",
    "batch_size=1\n",
    "extension='.mp3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ee157ad-de6d-4e80-97d5-d4f947ba97ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 971/971 [00:00<00:00, 1739.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "965"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_list = list()\n",
    "durations = 0\n",
    "max_sec = 10.0\n",
    "min_sec = 2.0\n",
    "\n",
    "# kspon_wavs = glob(os.path.join(kspon_path, '**', '*.wav'), recursive=True)\n",
    "# kspon_wavs = glob(os.path.join(kspon_path, '3.일상안부','dialog_01','001', '*.wav'), recursive=True)\n",
    "kspon_wavs = glob(os.path.join('*.mp3'), recursive=True)\n",
    "\n",
    "# print(kspon_wavs)\n",
    "random.seed(44)\n",
    "random.shuffle(kspon_wavs)\n",
    "\n",
    "remove_re = '[a-zA-Z0-9%]'\n",
    "\n",
    "for file in tqdm(kspon_wavs):\n",
    "    duration = librosa.get_duration(filename=file, sr=sr)\n",
    "    if (min_sec <= duration) and (max_sec >= duration):\n",
    "        text_path = file.replace(extension, \".txt\")\n",
    "        text = _read_txt_file(text_path)\n",
    "        text = special_filter(bracket_filter(text))\n",
    "        if re.findall(remove_re, text) == []:\n",
    "            text_list.append(text)\n",
    "            audio_list.append(file)\n",
    "            durations += duration\n",
    "    if durations >= 500*60*60:\n",
    "        break\n",
    "\n",
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50c39de6-eecd-49cf-aa25-890ea6d0353a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "965\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 965\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>내가 아는 한의원 소개해 줄까요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>대신 결과는 책임져야 해</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>오늘은 나도 한 이만 원 정도 복권을 사볼까</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>저 장난감은 얼마인가요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>어머님 낼 아침에 오신데</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>갈 때 꼭 영수증 챙겨서 가야 한다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>이 크래커 위에 참치 올려서 먹으면 맛있다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>안녕하세요 청년 피자입니다 무엇을 드릴까요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>오늘은 해가 쨍쨍해서 정말 기분이 좋은데요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>좀 믿고 먹일만한 간식 어디 없을까</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>토너 배송받으면 기존에 쓰던 거 드려야 되는 거야</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>내가 가방 들어줄까</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>나 아까 치킨 먹다 체한 것 같은데 잠깐 약국 좀 들려서 소화제 좀 사 가자</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>혼자 사니까 살만하니</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>일도 괜찮고 동료들도 너무 좋은 것 같애</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_dict = {\"text\": text_list}\n",
    "\n",
    "vocab_timit = Dataset.from_dict(text_dict)\n",
    "print(len(vocab_timit))\n",
    "print(vocab_timit)\n",
    "\n",
    "show_random_elements(vocab_timit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "648c87f8-97fd-4225-812f-fb98395e6ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f3ed58add748ffa108d74abeca8317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "56071"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_timit.to_csv('/wav2vec2/s-kr/fine-tune/dataset/aihub_results/mario_gec_testdata.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6314ff3f-f93d-4220-ac92-91be99aafd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>내가 아는 한의원 소개해 줄까요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>대신 결과는 책임져야 해</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>오늘은 나도 한 이만 원 정도 복권을 사볼까</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>저 장난감은 얼마인가요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>어머님 낼 아침에 오신데</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>960</td>\n",
       "      <td>요즘에 안성탕면이 유행이라면서 나도 안성탕면 좋아하는데 어디서 파는 거지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>961</td>\n",
       "      <td>진짜 오랜만에 만난 거 같네요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>962</td>\n",
       "      <td>절대 안 되니까 면허증 보여주세요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>963</td>\n",
       "      <td>그 보험 상담사는 나의 심기를 건드려서 나는 그 보험을 들지 않았어</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>964</td>\n",
       "      <td>아이가 있으면 집이 조용하질 않더라구</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>965 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                      text\n",
       "0             0                         내가 아는 한의원 소개해 줄까요\n",
       "1             1                             대신 결과는 책임져야 해\n",
       "2             2                  오늘은 나도 한 이만 원 정도 복권을 사볼까\n",
       "3             3                              저 장난감은 얼마인가요\n",
       "4             4                             어머님 낼 아침에 오신데\n",
       "..          ...                                       ...\n",
       "960         960  요즘에 안성탕면이 유행이라면서 나도 안성탕면 좋아하는데 어디서 파는 거지\n",
       "961         961                          진짜 오랜만에 만난 거 같네요\n",
       "962         962                        절대 안 되니까 면허증 보여주세요\n",
       "963         963     그 보험 상담사는 나의 심기를 건드려서 나는 그 보험을 들지 않았어\n",
       "964         964                      아이가 있으면 집이 조용하질 않더라구\n",
       "\n",
       "[965 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dada=pd.read_csv('/wav2vec2/s-kr/fine-tune/dataset/aihub_results/mario_gec_testdata.tsv')\n",
    "\n",
    "dada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43a2d60-f0de-4d9d-9212-7de6d259a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기까지만 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7608b890-bb70-4dd6-9750-b44f9e449191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4844c0dd-fe2b-4922-bc09-4723b78fa401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"text\"])\n",
    "    vocab = list(set(all_text))\n",
    "    \n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "vocabs = extract_all_chars(vocab_timit)\n",
    "vocab_list = list(set(vocabs[\"vocab\"][0]))\n",
    "\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b20644a-29bf-4e04-960f-9dfb6015ea81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95fd9425-1898-42ad-8f9f-baf655b2e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "vocab_path = os.path.join(dataset_path, 'broadcast_vocab.json')\n",
    "with open(vocab_path, 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9f4fa7-b18a-40c1-b81f-69aa5c13bae3",
   "metadata": {},
   "source": [
    "# Processor 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d16dcd8a-70ed-45ce-aadd-42bca5314ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Processor:\n",
       "- feature_extractor: Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000\n",
       "}\n",
       "\n",
       "- tokenizer: PreTrainedTokenizer(name_or_path='', vocab_size=1999, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'pad_token': '[PAD]'})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(vocab_path, unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=sr, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06526c-19b7-4db7-90d7-cbcd27c69257",
   "metadata": {},
   "source": [
    "# 학습 데이터셋 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c46af7ee-073f-4280-8317-eb7b662f1278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setproctitle import setproctitle\n",
    "setproctitle(\"aihub_Wav2Vec2 Dataset\")\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import Wav2Vec2Processor\n",
    "from kspon_preprocess import special_filter, bracket_filter, del_noise  # 특수 기호 제거하는 전처리 코드 함수\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import re\n",
    "\n",
    "input_values_list = list()\n",
    "input_length_list = list()\n",
    "labels_list = list()\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
    "remove_re = '[a-zA-Z0-9%]'\n",
    "\n",
    "def remove_special_characters(text: str) -> str:\n",
    "    text = special_filter(bracket_filter(text))\n",
    "    # text = re.sub(chars_to_ignore_regex, '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8e885be-b058-43e5-bad6-20c785306bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 376583/376583 [16:20<00:00, 384.11it/s]\n"
     ]
    }
   ],
   "source": [
    "for text, audio_path in tqdm(zip(text_list, audio_list), total=len(text_list)):\n",
    "    # audio, _ = sf.read(audio_path)\n",
    "    audio, _ = librosa.load(audio_path, sr=sr)\n",
    "    non_silence_indices = del_noise(audio, top_db=30)  # del_noise 함수를 통해 노이즈 제거\n",
    "    audio = np.concatenate([audio[start:end] for start, end in non_silence_indices])\n",
    "    if audio.ndim > 1:\n",
    "        audio = np.delete(audio, 1, axis=1)\n",
    "        audio = audio.reshape(-1)\n",
    "    input_value = processor(audio, sampling_rate=sr).input_values[0]\n",
    "    input_values_list.append(input_value)\n",
    "    input_length_list.append(len(input_value))\n",
    "    text = remove_special_characters(text)\n",
    "    with processor.as_target_processor():\n",
    "        labels_list.append(processor(text).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30a7f2f6-2397-40bf-8983-7cf98693390c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376583 376583 376583\n",
      "[-0.01602839 -0.01658881 -0.00341888 ...  0.07335903  0.06299122\n",
      "  0.04645874] (50688,)\n",
      "50688\n",
      "[111, 256, 1616, 1936, 1742, 256, 378, 1359, 256, 1620, 955, 256, 582, 1393, 256, 1350, 154, 256, 648, 256, 1486, 732]\n"
     ]
    }
   ],
   "source": [
    "print(len(input_values_list), len(input_length_list), len(labels_list))\n",
    "\n",
    "print(input_values_list[0], input_values_list[0].shape)\n",
    "print(input_length_list[0])\n",
    "print(labels_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6ef65d3-ff8b-4856-9cee-e4d5f17d5455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372817\n",
      "3766\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_rate = 0.99\n",
    "train_idx = int(train_rate * len(input_values_list))\n",
    "\n",
    "train_df = pd.DataFrame({'input_values': input_values_list[:train_idx], 'input_length': input_length_list[:train_idx], 'labels': labels_list[:train_idx]})\n",
    "test_df = pd.DataFrame({'input_values': input_values_list[train_idx:], 'input_length': input_length_list[train_idx:], 'labels': labels_list[train_idx:]})\n",
    "\n",
    "print(len(train_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db647d1-dde9-474b-8543-0e3d10794ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_timit = Dataset.from_pandas(train_df)\n",
    "test_timit = Dataset.from_pandas(test_df)\n",
    "\n",
    "print(len(train_timit))\n",
    "print(train_timit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe032643-50a1-439c-98da-77aff3c7e527",
   "metadata": {},
   "source": [
    "# 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cf3d74-01bb-4e95-ba5a-e9965110edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import IPython.display as ipd\n",
    "# import numpy as np\n",
    "# import random\n",
    "\n",
    "# rand_int = random.randint(0, len(train_timit))\n",
    "\n",
    "# ipd.Audio(data=np.asarray(train_timit[rand_int][\"input_values\"]), rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d2509-bebb-4208-9264-ee5ee0fdac46",
   "metadata": {},
   "source": [
    "# Train 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2563eea1-d0c0-4482-b4a2-a62e69ce12fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b79b68-df9c-46f4-91f4-7b01f65e3737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "cer_metric = load_metric(\"cer\")\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe7ca1c-23c6-44e9-97ce-0dd739c05ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    # wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f69d93-2e0b-4c4e-8b41-33123879994c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Model, Wav2Vec2ForCTC, Wav2Vec2Config\n",
    "\n",
    "configuration = Wav2Vec2Config(num_attention_heads=num_attention_heads)\n",
    "model = Wav2Vec2Model(configuration)\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    # \"facebook/wav2vec2-large-xlsr-53\",\n",
    "    \"/wav2vec2/s-kr/fine-tune/dataset/results/checkpoint-175000\", # 새로만든 보캡 옮겨야함.\n",
    "    # gradient_checkpointing=True,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    configuration,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size = len(processor.tokenizer)\n",
    ")\n",
    "\n",
    "# print(model.config)\n",
    "# print(\"=\" * 100)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66f2e40-a06e-418e-a6d6-98d299c39f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba8d765-e99c-4cf4-bdb1-65c3b2e97dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = os.path.join(dataset_path, \"aihub_results\")\n",
    "\n",
    "number = 1000\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=output_dir,           \n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=batch_size,\n",
    "  per_device_eval_batch_size=batch_size,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=30,\n",
    "  fp16=True,\n",
    "  gradient_checkpointing=True,\n",
    "  save_steps=number,\n",
    "  eval_steps=number,\n",
    "  logging_steps=number,\n",
    "  learning_rate=1e-4,\n",
    "  log_on_each_node=True,\n",
    "  weight_decay=0.005,\n",
    "  warmup_steps=number,\n",
    "  eval_accumulation_steps=1,\n",
    "  save_total_limit=5,\n",
    "  load_best_model_at_end=True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46b1a4b-f875-4d81-9710-12db82072cf1",
   "metadata": {},
   "source": [
    "# Train 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f2a608-c851-40e9-a450-61c9cb865acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, EarlyStoppingCallback\n",
    "from setproctitle import setproctitle\n",
    "setproctitle(\"wav2vec2-south-ft-aihub-mario\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_timit,\n",
    "    eval_dataset=test_timit,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63763a41-8f28-498c-a682-d235d1540973",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    for obj in trainer.state.log_history:\n",
    "        print(obj)\n",
    "        \n",
    "    trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b6b60-d943-4ec7-8bfb-dc8e96f73e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a32696-e2fa-48ff-9bad-4beaa0db2bec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
